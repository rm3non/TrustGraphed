Great progress â€” you've successfully integrated the **content declaration dropdown** and connected it to the backend. Thatâ€™s a huge protocol integrity milestone âœ…

However, if **all content types now result in a low score** (regardless of user declaration), that points to an **overcorrection in the scoring logic** â€” possibly in your `cce.py` or `score_engine.py` penalty application.

Letâ€™s analyze and fix it:

---

## âš ï¸ SYMPTOM:

> All assertion types (Original, AI-generated, Copied, Mixed, Unsure) return **very low TrustScores**, even for clean content.

---

## ğŸ¯ LIKELY ROOT CAUSES

### 1. **Global penalty is being applied regardless of declaration**

Check in `cce.py` or `score_engine.py` if something like this exists:

```python
if assertion_type != "original":
    score *= 0.5
```

If this logic doesn't differentiate between â€œcopiedâ€ vs. â€œmixedâ€ vs. â€œAI,â€ it applies a **flat penalty** â€” which is too harsh and defeats the purpose of nuanced intent scoring.

---

### 2. **Penalty stack is multiplicative instead of weighted**

Letâ€™s say:

```python
score *= 0.75  # AI
score *= 0.7   # No citations
score *= 0.85  # Low contradiction
```

This will result in:
`final_score = base_score * 0.75 * 0.7 * 0.85 = ~0.45x base`

â†’ Even good content will tank below 50.

---

### 3. **Assertion type overrides are missing soft boosts**

If someone selects **â€œPartially AI-generatedâ€** and provides evidence, we should reward them for honesty by:

* Applying a **smaller penalty**
* Or allowing override if citation or attribution is present

---

## âœ… HOW TO FIX IT

### ğŸ”§ In `cce.py` â€“ Adjust Penalty Logic:

```python
def apply_assertion_penalty(score, assertion_type):
    if assertion_type == "original":
        return score  # no penalty
    elif assertion_type == "ai":
        return score * 0.85  # mild penalty
    elif assertion_type == "copied":
        return score * 0.75
    elif assertion_type == "mixed":
        return score * 0.8
    elif assertion_type == "unsure":
        return score * 0.6  # harshest
    else:
        return score * 0.7  # fallback
```

â†’ Then call this before returning your confidence score:

```python
final_score = apply_assertion_penalty(base_score, assertion_type)
```

---

### âœ… Add Positive Weights (Optional, but Ideal)

If citations are found, or metadata is valid:

```python
if signals["citations"] > 2:
    final_score += 5  # small boost

if "author" in signals:
    final_score += 5
```

â†’ Cap score at 100 to avoid overflow.

---

## ğŸ” Also Fix in UI (Optional)

On the frontend:

* Show adjusted score with badge:
  *"User declared this content as â€˜Partially AI-generatedâ€™. Score adjusted accordingly."*

---

## ğŸ“Œ Final Recommendation

| Fix Type                    | Priority    | Impact                                     |
| --------------------------- | ----------- | ------------------------------------------ |
| Adjust penalties per intent | âœ… Critical  | Restores logic fairness                    |
| Normalize scoring tiers     | âœ… High      | Prevents score suppression                 |
| Cap + floor logic           | ğŸŸ¡ Medium   | Stops negative scores or runaway inflation |
| Add positive signal boosts  | ğŸŸ¢ Optional | Encourages honesty and good formatting     |

---

Would you like me to give you:

* The **exact code diff for `cce.py` and `score_engine.py`** to fix this?
* A JSON-based **scoring logic map** you can use to tweak live?

You're very close to protocol-grade behavior â€” this is the final tuning pass.
